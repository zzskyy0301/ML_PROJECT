{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG&mobilenet_POSTER_advGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "15tVISPKVExNRGAjqtlO8zIqp0IZrPJkv",
      "authorship_tag": "ABX9TyNYl48zdkYMBLWjL6A/rJQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "314645f21b30493395e44c5fcde2f48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eb7d64dadb74b2d85fc005e00c48345",
              "IPY_MODEL_2d62dadc00a44f2c8e7efb3e40d5b9a7",
              "IPY_MODEL_85967c8f04b44d3182f8c4b11378ec5c"
            ],
            "layout": "IPY_MODEL_694064505c5b4934b3044f34628fbe08"
          }
        },
        "3eb7d64dadb74b2d85fc005e00c48345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8c6bd5bc7064a1fa076d7d43adfc16b",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c7d5275a0d41f0856afa994ffee031",
            "value": "100%"
          }
        },
        "2d62dadc00a44f2c8e7efb3e40d5b9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088d7153d25346d1a154b055f3cdb748",
            "max": 22139423,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32139889ff504012abab499f1f74bec4",
            "value": 22139423
          }
        },
        "85967c8f04b44d3182f8c4b11378ec5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511411527cd44a0f8a0ad6bc1e8cd65e",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6b23c475a647ea8d78ca42459f2de6",
            "value": " 21.1M/21.1M [00:00&lt;00:00, 68.5MB/s]"
          }
        },
        "694064505c5b4934b3044f34628fbe08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8c6bd5bc7064a1fa076d7d43adfc16b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c7d5275a0d41f0856afa994ffee031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "088d7153d25346d1a154b055f3cdb748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32139889ff504012abab499f1f74bec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "511411527cd44a0f8a0ad6bc1e8cd65e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6b23c475a647ea8d78ca42459f2de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzskyy0301/ML_PROJECT/blob/main/VGG%26mobilenet_POSTER_advGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataloader**"
      ],
      "metadata": {
        "id": "jzVJX80vx39j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from collections import defaultdict, namedtuple\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "import glob as glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "#@title GTSRB_Net\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "4ChXjv6dX5dd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/adversarial/GTSRB_Final_Training_Images.zip -d /content/drive/MyDrive/adversarial\n",
        "#!unzip -d"
      ],
      "metadata": {
        "id": "m9iIniMDPSbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv -P /content/drive/MyDrive/adversarial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJSirXUaXbzz",
        "outputId": "fa7f60d3-3696-4521-a5ca-2933824d630e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-18 08:12:59--  https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 999 [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/adversarial/signnames.csv’\n",
            "\n",
            "signnames.csv       100%[===================>]     999  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-18 08:12:59 (37.3 MB/s) - ‘/content/drive/MyDrive/adversarial/signnames.csv’ saved [999/999]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training_annotations(source_path):\n",
        "    annotations = {}\n",
        "    for c in range(0,43):\n",
        "        filename = os.path.join(source_path, format(c, '05d'), 'GT-' + format(c, '05d') + '.csv')\n",
        "        #print(filename)\n",
        "        with open(filename) as f:\n",
        "          reader = csv.reader(f, delimiter=';')\n",
        "          next(reader) # skip header\n",
        "\n",
        "          # loop over all images in current annotations file\n",
        "          for row in reader:\n",
        "              #print(row)\n",
        "              filename = row[0] # filename is in the 0th column\n",
        "              width = float(row[1])\n",
        "              height = float(row[2])\n",
        "              x1=float(row[3])/float(width)\n",
        "              y1=float(row[4])/float(height)\n",
        "              x2=float(row[5])/float(width)\n",
        "              y2=float(row[6])/float(height)\n",
        "              label = int(row[7]) # label is in the 7th column\n",
        "        \n",
        "              annotations[filename]={'width':width,\n",
        "                               'height':height,\n",
        "                               'x1':x1,\n",
        "                               'y1':y1,\n",
        "                               'x2':x2,\n",
        "                               'y2':y2,\n",
        "                               'label':label}\n",
        "    return annotations\n"
      ],
      "metadata": {
        "id": "wF_wYWCmX4m6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = load_training_annotations('/content/drive/MyDrive/adversarial/GTSRB/Final_Training/Images')"
      ],
      "metadata": {
        "id": "gqjqB1pLYNjt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GTSRB_dataloader(data.Dataset):\n",
        "    def __init__(self, opt):\n",
        "        super(GTSRB_dataloader, self).__init__()\n",
        "        self.image_list = []\n",
        "        self.root = opt['dataroot']\n",
        "        self.batch_size = opt['batch_size']\n",
        "        self.transform = opt['transform']\n",
        "        self.mode = opt['mode']\n",
        "        self.annotations = opt['annotations']\n",
        "        self.tranform = opt['transform']\n",
        "        self._assertValid()\n",
        "        #self.epoch = opt['epoch']\n",
        "        #self.img_list = os.listdir(opt.dataroot) #data root: .training/Image/\n",
        "        #self.num_labels = 2\n",
        "        #self.mean = [0.485, 0.456, 0.406]\n",
        "        #self.std = [0.229, 0.224, 0.225]\n",
        "        #self._assertValid()\n",
        "    \n",
        "    def _assertValid(self):\n",
        "      for files in os.listdir(self.root):\n",
        "          for file in os.listdir(os.path.join(self.root,files)):\n",
        "            if file.endswith(\".ppm\"):\n",
        "                 self.image_list.append(os.path.join(self.root, files,file))\n",
        "      \n",
        "      assert len(self.image_list)>0, \"Not able to locate image data\"\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        #img_path = os.listdir(os.path.join(),suffix == 'ppm')\n",
        "        img = Image.open(self.image_list[index])\n",
        "        img_name =os.path.basename(self.image_list[index])\n",
        "        if img is None:\n",
        "            return {}\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "           \n",
        "        img_anno = self.annotations[img_name]\n",
        "        img_anno ['img_name'] = img_name\n",
        "        img_anno ['width'] = img.shape[1]\n",
        "        img_anno ['height']= img.shape[2]\n",
        "        return {'image': img, \n",
        "                \n",
        "                'annotation':img_anno}\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)"
      ],
      "metadata": {
        "id": "7H5KFAT_llVk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IGLmaCEvBTHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try the loader"
      ],
      "metadata": {
        "id": "mhg6-cAThGfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opt = {\"dataroot\": '/content/drive/MyDrive/adversarial/GTSRB/Final_Training/Images', \n",
        "   \"batch_size\": 1,\n",
        "   \"transform\": None,\n",
        "   \"mode\": \"train\", \n",
        "   \"annotations\": annotations,\n",
        "   'transform': transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))])}\n",
        "dataset = GTSRB_dataloader(opt)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = opt[\"batch_size\"], shuffle=True)\n",
        "for idx, batch in enumerate(dataloader):\n",
        "    if idx ==0:\n",
        "      img = batch['image']\n",
        "      img = img.numpy()\n",
        "      img = np.squeeze(img, axis=0)\n",
        "      img = np.transpose(img, (1,2,0))\n",
        "      xmin = batch['annotation']['width']*batch['annotation']['x1']\n",
        "      ymin = batch['annotation']['height']*batch['annotation']['y1']\n",
        "      xmax = batch['annotation']['width']*batch['annotation']['x2']\n",
        "      ymax = batch['annotation']['height']*batch['annotation']['y2']\n",
        "      print(xmin,ymin,xmax,ymax)\n",
        "      fig, ax = plt.subplots()\n",
        "      ax.imshow(img)\n",
        "      rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "      plt.show()\n",
        "      # cv2.rectangle(img,\n",
        "      #               (batch['annotation']['width']*batch['annotation']['x1'],batch['annotation']['height']*batch['annotation']['y1']),\n",
        "      #               (batch['annotation']['width']*batch['annotation']['x2'],batch['annotation']['height']*batch['annotation']['y2']),\n",
        "      #               (255,0,0),2)\n",
        "      #plt.imshow(img)\n",
        "      \n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "r0NVwsrw_j6j",
        "outputId": "fcc7fea6-e897-4b9c-caa9-683c45e8e602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.], dtype=torch.float64) tensor([3.9184], dtype=torch.float64) tensor([28.], dtype=torch.float64) tensor([28.7347], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py:789: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  points = np.array(args, dtype=float).reshape(2, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models.py**"
      ],
      "metadata": {
        "id": "LTr3df1T_lrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nclasses = 43 # GTSRB as 43 classes\n",
        "\n",
        "class GTSRB_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GTSRB_Net, self).__init__()\n",
        "        \n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv2d(3, 100, kernel_size=5)\n",
        "        self.bn1 = nn.BatchNorm2d(100)\n",
        "        self.conv2 = nn.Conv2d(100, 150, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(150)\n",
        "        self.conv3 = nn.Conv2d(150, 250, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(250)\n",
        "        self.conv_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(250*2*2, 350)\n",
        "        self.fc2 = nn.Linear(350, nclasses)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "            )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * 4 * 4, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "            )\n",
        "   \n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 10 * 4 * 4)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # Perform forward pass\n",
        "        x = self.bn1(F.max_pool2d(F.leaky_relu(self.conv1(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = self.bn2(F.max_pool2d(F.leaky_relu(self.conv2(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = self.bn3(F.max_pool2d(F.leaky_relu(self.conv3(x)),2))\n",
        "        x = self.conv_drop(x)\n",
        "        x = x.view(-1, 250*2*2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        test = F.log_softmax(x, dim=1)\n",
        "        #print(test.shape)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "5OUXNjJFWtln",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOURCE:https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/dataset.py"
      ],
      "metadata": {
        "id": "tGf7IQp3Xf0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\" \n",
        "Information about architecture config:\n",
        "Tuple is structured by (filters, kernel_size, stride) \n",
        "Every conv is a same convolution. \n",
        "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
        "\"S\" is for scale prediction block and computing the yolo loss\n",
        "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
        "\"\"\"\n",
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.1)\n",
        "        self.use_bn_act = bn_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_bn_act:\n",
        "            return self.leaky(self.bn(self.conv(x)))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for repeat in range(num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
        "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        self.use_residual = use_residual\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            if self.use_residual:\n",
        "                x = x + layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ScalePrediction(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.pred = nn.Sequential(\n",
        "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "            CNNBlock(\n",
        "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
        "            ),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            self.pred(x)\n",
        "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=80):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.layers = self._create_conv_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []  # for each scale\n",
        "        route_connections = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ScalePrediction):\n",
        "                outputs.append(layer(x))\n",
        "                continue\n",
        "\n",
        "            x = layer(x)\n",
        "\n",
        "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "                route_connections.append(x)\n",
        "\n",
        "            elif isinstance(layer, nn.Upsample):\n",
        "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "                route_connections.pop()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _create_conv_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for module in config:\n",
        "            if isinstance(module, tuple):\n",
        "                out_channels, kernel_size, stride = module\n",
        "                layers.append(\n",
        "                    CNNBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=1 if kernel_size == 3 else 0,\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "            elif isinstance(module, list):\n",
        "                num_repeats = module[1]\n",
        "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
        "\n",
        "            elif isinstance(module, str):\n",
        "                if module == \"S\":\n",
        "                    layers += [\n",
        "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
        "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
        "                    ]\n",
        "                    in_channels = in_channels // 2\n",
        "\n",
        "                elif module == \"U\":\n",
        "                    layers.append(nn.Upsample(scale_factor=2),)\n",
        "                    in_channels = in_channels * 3\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_classes = 20\n",
        "    IMAGE_SIZE = 64\n",
        "    model = YOLOv3(num_classes=num_classes)\n",
        "    x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
        "    out = model(x)\n",
        "    print(out)\n",
        "    assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
        "    assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
        "    assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
        "    print(\"Success!\")"
      ],
      "metadata": {
        "cellView": "code",
        "id": "DjTglgRpbkJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ2LudJKVbR0",
        "outputId": "084cc364-b6c0-49c8-8bc2-e5c41105fdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 2, 2, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VGG-16** "
      ],
      "metadata": {
        "id": "m2nUatLbXFBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ub-bHWjOXiqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D&G"
      ],
      "metadata": {
        "id": "UEEUmaGwuVJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Poster Discriminator & Generator\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # MNIST: 1*28*28\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*13*13\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # MNIST:1*28*28\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 28 x 28\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "OhKsqZ-u_M_S",
        "cellView": "code"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **advGAN.py**"
      ],
      "metadata": {
        "id": "YS7HlcEK_zjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RkCuzF_M7ylI",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title Poster Attack\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "models_path = './models/'\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "        if not os.path.exists(models_path):\n",
        "            os.makedirs(models_path)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "\n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -0.3, 0.3) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "\n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            onehot_labels = torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "\n",
        "            # C&W loss function\n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 10\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb\n",
        "            loss_G.backward()\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        for epoch in range(1, epochs+1):\n",
        "            print(epoch%1==0)\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images ,target = data['image'],data['annotation']\n",
        "                labels = target['label']\n",
        "      \n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "                if i % 500 == 0:\n",
        "                    print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
        "                        epoch, i * len(data), len(train_dataloader.dataset),\n",
        "                        loss_adv_sum))\n",
        "                \n",
        "            # # print statistics\n",
        "            # num_batch = len(train_dataloader)\n",
        "            # print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "            #  \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            #       (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "            #        loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "\n",
        "            # save generator\n",
        "            if epoch%1==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **main.py**"
      ],
      "metadata": {
        "id": "nzodFq1Q_r_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=3\n",
        "epochs = 3\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "model_num_labels = 43\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "\n",
        "#VGG-16\n",
        "vgg16 =  models.vgg16(pretrained=True)\n",
        "# Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "# Newly created modules have require_grad=True by default\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, model_num_labels)]) # Add our layer with 4 outputs\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "#mobilenet\n",
        "mobileNet =models.mobilenet_v3_large(pretrained=True,progress=True)    \n",
        "mobileNet.classifier[-1] = nn.Linear(1280, model_num_labels)\n"
      ],
      "metadata": {
        "id": "4nvGm7qC_U0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "314645f21b30493395e44c5fcde2f48d",
            "3eb7d64dadb74b2d85fc005e00c48345",
            "2d62dadc00a44f2c8e7efb3e40d5b9a7",
            "85967c8f04b44d3182f8c4b11378ec5c",
            "694064505c5b4934b3044f34628fbe08",
            "a8c6bd5bc7064a1fa076d7d43adfc16b",
            "b2c7d5275a0d41f0856afa994ffee031",
            "088d7153d25346d1a154b055f3cdb748",
            "32139889ff504012abab499f1f74bec4",
            "511411527cd44a0f8a0ad6bc1e8cd65e",
            "7b6b23c475a647ea8d78ca42459f2de6"
          ]
        },
        "id": "GbrvzI2s7oK7",
        "outputId": "4aec60db-57c5-4c2f-f06d-43a9412627fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/21.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "314645f21b30493395e44c5fcde2f48d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targeted_model = vgg16\n",
        "targeted_model.to(device)\n",
        "targeted_model.eval()"
      ],
      "metadata": {
        "id": "9ZsL4FJy32EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\"dataroot\": '/content/drive/MyDrive/adversarial/GTSRB/Final_Training/Images', \n",
        "   \"batch_size\": 32,\n",
        "   \"transform\": None,\n",
        "   \"mode\": \"train\", \n",
        "   \"annotations\": annotations,\n",
        "   'transform': transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))])}\n",
        "\n",
        "dataset =GTSRB_dataloader(opt)\n",
        "dataloader = DataLoader(dataset, batch_size=opt['batch_size'], shuffle=True, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "nbjE-E7VOMg1",
        "outputId": "81e4b2f4-9125-4eb2-ec48-7fb97374fec2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "torch.Size([32, 43])\n",
            "torch.Size([32, 43])\n",
            "Train Epoch: 1 [0/39209]\tLoss: 0.012349\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f3a4f6792e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                           BOX_MAX)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0madvGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-6d78ed64af14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, epochs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mloss_perturb_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mloss_adv_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train target model (GTSRB_NET)**"
      ],
      "metadata": {
        "id": "hDLRpOoH_ZI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    training_loss = 0\n",
        "    for batch_idx, image in enumerate(train_loader, 0):\n",
        "        data ,target = image['image'],image['annotation']\n",
        "        target = target['label']\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        max_index = output.max(dim = 1)[1]\n",
        "        correct += (max_index == target).sum()\n",
        "        training_loss += loss\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss per example: {:.6f}\\tLoss: {:.6f}'.format(\n",
        "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                        100. * batch_idx / len(train_loader), loss.data.item()/(batch_size * 10),loss.data.item()))\n",
        "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "                training_loss / len(train_loader.dataset), correct, len(train_loader.dataset),\n",
        "                100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for batch_idx, image in val_loader:\n",
        "        with torch.no_grad():\n",
        "            data ,target = image['image'],image['annotation']\n",
        "            target = target['label']\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    scheduler.step(np.around(validation_loss,2))\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "###################################### main functions #################################\n",
        "image_nc = 3\n",
        "batch_size = 8\n",
        "num_epoch = 1\n",
        "if torch.cuda.is_available():\n",
        "    use_gpu = True\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "\tuse_gpu = False\n",
        "\tprint(\"Using CPU\")\n",
        "mnist_dataset = GTSRB_dataloader(opt)\n",
        "train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "val_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "model = GTSRB_Net().to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()),lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=5,factor=0.5,verbose=True)\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "    train(epoch)\n",
        "    validation()\n",
        "    model_file = 'model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '. Run `python evaluate.py ' + model_file + '` to generate the Kaggle formatted csv file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "-GSyfL2XfqzO",
        "outputId": "384d8b83-d795-4c0b-8164-c3ee4cf7cb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "CUDA Available:  True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4278: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4216: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/39209 (0%)]\tLoss per example: 0.050289\tLoss: 4.023096\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-10cc16c2fd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-10cc16c2fd05>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 3\n",
        "    batch_size = 8\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    mnist_dataset = GTSRB_dataloader(opt)\n",
        "    train_dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "    # training the target model\n",
        "    target_model = GTSRB_Net().to(device)\n",
        "    target_model.train()\n",
        "    #opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad,target_model.parameters()),lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=5,factor=0.5,verbose=True)\n",
        "    epochs = 40\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            #data format: 'img','annotations'\n",
        "            \n",
        "\n",
        "            train_imgs, train_annotations = data\n",
        "            train_labels = train_annotations['label']\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.nll_loss(logits_model, train_labels)\n",
        "\n",
        "             \n",
        "        \n",
        "            loss_epoch += loss_model\n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "\n",
        "        print('loss in epoch %d: %f' % (epoch, loss_epoch.item()))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './MNIST_target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # MNIST test dataset\n",
        "    mnist_dataset_test = torchvision.datasets.MNIST('./dataset', train=False, transform=transforms.ToTensor(), download=True)\n",
        "    test_dataloader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(num_correct.item()/len(mnist_dataset_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "lkA8jUa8_eh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test adversarial examples**"
      ],
      "metadata": {
        "id": "GyyTpMo48jq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=3\n",
        "batch_size = 128\n",
        "\n",
        "gen_input_nc = image_nc\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "# load the pretrained model\n",
        "pretrained_model = \"/content/drive/MyDrive/adversarial/model_40.pth\"\n",
        "target_model = GTSRB_Net().to(device)\n",
        "target_model.load_state_dict(torch.load(pretrained_model))\n",
        "target_model.eval()\n",
        "\n",
        "# load the generator of adversarial examples\n",
        "pretrained_generator_path = '/content/drive/MyDrive/adversarial/netG_epoch_1.pth'\n",
        "pretrained_G = Generator(gen_input_nc, image_nc).to(device)\n",
        "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
        "pretrained_G.eval()\n",
        "\n",
        "# test adversarial examples in MNIST training dataset\n",
        "opt = {\"dataroot\": '/content/drive/MyDrive/adversarial/GTSRB/Final_Training/Images', \n",
        "   \"batch_size\": 1,\n",
        "   \"transform\": None,\n",
        "   \"mode\": \"train\", \n",
        "   \"annotations\": annotations,\n",
        "   'transform': transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()])}\n",
        "dataset = GTSRB_dataloader(opt)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = opt[\"batch_size\"], shuffle=False)\n",
        "#dataset = torchvision.datasets.MNIST('./dataset', train=True, transform=transforms.ToTensor(), download=True)\n",
        "#train_dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "num_correct = 0\n",
        "for i, image in enumerate(dataloader, 0):\n",
        "    test_img,test_target = image['image'],image['annotation']\n",
        "    test_label = test_target['label']\n",
        "    test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "    perturbation = pretrained_G(test_img)\n",
        "    #print(perturbation.shape,test_img.shape)\n",
        "    perturbation = torch.clamp(perturbation, -0.3, 0.3)\n",
        "\n",
        "    \n",
        "    adv_img = perturbation + test_img\n",
        "    adv_img = torch.clamp(adv_img, 0, 1)\n",
        "    if i ==100: \n",
        "      img = adv_img.detach().cpu().numpy()\n",
        "      img = np.squeeze(img, axis=0)\n",
        "      img = np.transpose(img, (1,2,0))\n",
        "\n",
        "      img2 = test_img.detach().cpu().numpy()\n",
        "      img2 = np.squeeze(img2, axis=0)\n",
        "      img2 = np.transpose(img2, (1,2,0))\n",
        "\n",
        "      figure, axis = plt.subplots(2, 1)\n",
        "      axis[0].imshow(img)\n",
        "      axis[1].imshow(img2)\n",
        "      break\n",
        "\n",
        "\n",
        "    pred_lab = torch.argmax(target_model(adv_img),1)\n",
        "    num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "print('GTSRB training dataset:')\n",
        "print('num_correct: ', num_correct.item())\n",
        "print('accuracy of adv imgs in training set: %f\\n'%(num_correct.item()/len(dataset)))\n",
        "\n",
        "# test adversarial examples in MNIST testing dataset\n",
        "# mnist_dataset_test = torchvision.datasets.MNIST('./dataset', train=False, transform=transforms.ToTensor(), download=True)\n",
        "# test_dataloader = DataLoader(mnist_dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "# num_correct = 0\n",
        "# for i, data in enumerate(test_dataloader, 0):\n",
        "#     test_img, test_label = data\n",
        "#     test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "#     perturbation = pretrained_G(test_img)\n",
        "#     perturbation = torch.clamp(perturbation, -0.3, 0.3)\n",
        "#     adv_img = perturbation + test_img\n",
        "#     adv_img = torch.clamp(adv_img, 0, 1)\n",
        "#     pred_lab = torch.argmax(target_model(adv_img),1)\n",
        "#     num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "# print('num_correct: ', num_correct.item())\n",
        "# print('accuracy of adv imgs in testing set: %f\\n'%(num_correct.item()/len(mnist_dataset_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Hrts1job8nrw",
        "outputId": "6ec9595b-738f-479b-fd81-1e8a5465e297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available:  True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4278: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4216: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GTSRB training dataset:\n",
            "num_correct:  0\n",
            "accuracy of adv imgs in training set: 0.000000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Attack Success Rate Calculation**"
      ],
      "metadata": {
        "id": "9eheiKWBQFRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzVz7vn3DB8n",
        "outputId": "6486242e-54c0-4cb9-97f5-867f40ca77bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39209"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}